
## ArrayList扩容机制分析

ArrayList有三个构造函数 , 无参构造会将赋值一个空数组 , 有参容量构造会创建指定容量的数组 , 有参集合构造会遍历集合拷贝集合

分析扩容 : 

在调用add之后会调用ensureCapacityInternal(size+1) ,
在这个方法中又会调用到计算所需容量是多少 , 如果是空数组 , 那么会返回10 , 否则返回需要的容量 , 
得到了需要的容量 , 那么需要比较目前数组的容量是否满足需要的容量 , 如果不满足 , 那么就需要调用grow方法进行扩容

1. 调用add

```java
public boolean add(E e) {
    // 加元素之前，先调用ensureCapacityInternal方法
    ensureCapacityInternal(size + 1);  // Increments modCount!!
    // 这里看到ArrayList添加元素的实质就相当于为数组赋值
    elementData[size++] = e;
    return true;
}
```

2. 计算容量并且判断是否需要grow

```java
// 根据给定的最小容量和当前数组元素来计算所需容量。
private static int calculateCapacity(Object[] elementData, int minCapacity) {
    // 如果当前数组元素为空数组（初始情况），返回默认容量和最小容量中的较大值作为所需容量
    if (elementData == DEFAULTCAPACITY_EMPTY_ELEMENTDATA) {
        return Math.max(DEFAULT_CAPACITY, minCapacity);
    }
    // 否则直接返回最小容量
    return minCapacity;
}

// 确保内部容量达到指定的最小容量。
private void ensureCapacityInternal(int minCapacity) {
    ensureExplicitCapacity(calculateCapacity(elementData, minCapacity));
}
```

```java
//判断是否需要扩容
private void ensureExplicitCapacity(int minCapacity) {
    modCount++;
    //判断当前数组容量是否足以存储minCapacity个元素
    if (minCapacity - elementData.length > 0)
        //调用grow方法进行扩容
        grow(minCapacity);
}
```

3. grow方法中先扩容成1.5倍 , 然后判断是否满足最小所需容量 , 如果不满足 , 那么把最小所需容量当做数组的新容量 , 如果新容量大于MAX_ARRAY_SIZE (int类型最大值-8 ) , 进入hugeCapacity方法

```java
/**
 * 要分配的最大数组大小
 */
private static final int MAX_ARRAY_SIZE = Integer.MAX_VALUE - 8;

/**
 * ArrayList扩容的核心方法。
 */
private void grow(int minCapacity) {
    // oldCapacity为旧容量，newCapacity为新容量
    int oldCapacity = elementData.length;
    // 将oldCapacity 右移一位，其效果相当于oldCapacity /2，
    // 我们知道位运算的速度远远快于整除运算，整句运算式的结果就是将新容量更新为旧容量的1.5倍，
    int newCapacity = oldCapacity + (oldCapacity >> 1);

    // 然后检查新容量是否大于最小需要容量，若还是小于最小需要容量，那么就把最小需要容量当作数组的新容量，
    if (newCapacity - minCapacity < 0)
        newCapacity = minCapacity;

    // 如果新容量大于 MAX_ARRAY_SIZE,进入(执行) `hugeCapacity()` 方法来比较 minCapacity 和 MAX_ARRAY_SIZE，
    // 如果minCapacity大于最大容量，则新容量则为`Integer.MAX_VALUE`，否则，新容量大小则为 MAX_ARRAY_SIZE 即为 `Integer.MAX_VALUE - 8`。
    if (newCapacity - MAX_ARRAY_SIZE > 0)
        newCapacity = hugeCapacity(minCapacity);

    // minCapacity is usually close to size, so this is a win:
    elementData = Arrays.copyOf(elementData, newCapacity);
}
```

4. hugeCapacity方法会判断当前容量是否溢出 , 溢出就抛出OOM异常 , 否则如果最小容量大于int最大值 , 那么赋值容量为最大值 , 否则赋值为int最大值-8

```java
private static int hugeCapacity(int minCapacity) {
    if (minCapacity < 0) // overflow
        throw new OutOfMemoryError();
    // 对minCapacity和MAX_ARRAY_SIZE进行比较
    // 若minCapacity大，将Integer.MAX_VALUE作为新数组的大小
    // 若MAX_ARRAY_SIZE大，将MAX_ARRAY_SIZE作为新数组的大小
    // MAX_ARRAY_SIZE = Integer.MAX_VALUE - 8;
    return (minCapacity > MAX_ARRAY_SIZE) ?
        Integer.MAX_VALUE :
        MAX_ARRAY_SIZE;
}
```

5. 最后会调用Arrays.copyOf方法复制数组 , 然后添加元素即可

## HashMap的put方法执行流程

![](Attachments/Images/Pasted%20image%2020240329224950.png)

1、判断键值对数组table\[i]是否为空（null）或者length=0，是的话就执行resize()方法进行扩容。

2、不是就根据键值key计算hash值得到插入的数组索引i。

3、判断table\[i]\==null，如果是true，直接新建节点进行添加，如果是false，判断table\[i]的首个元素是否和key一样，一样就直接覆盖。

4、判断table\[i]是否为treenode，即判断是否是红黑树，如果是红黑树，直接在树中插入键值对。

5、如果不是treenode，开始遍历链表，判断链表长度是否大于8，如果大于8就转成红黑树，在树中执行插入操作，如果不是大于8，就在链表中执行插入；在遍历过程中判断key是否存在，存在就直接覆盖对应的value值。

6、插入成功后，就需要判断实际存在的键值对数量size是否超过了最大容量threshold，如果超过了，执行resize方法进行扩容。


## volatile原理

原子性 : 不满足

**可见性 :** 

JMM中规定所有的变量都存储在主内存（Main Memory）中，每条线程都有自己的工作内存（Work Memory），线程的工作内存中保存了该线程所使用的变量的从主内存中拷贝的副本。线程对于变量的读、写都必须在工作内存中进行，而不能直接读、写主内存中的变量。同时，本线程的工作内存的变量也无法被其他线程直接访问，必须通过主内存完成。

`volatile`定义：

- 每个处理器会通过嗅探总线上的数据来查看自己的数据是否过期，一旦处理器发现自己缓存对应的内存地址被修改，就会将当前处理器的缓存设为无效状态。此时，如果处理器需要获取这个数据需重新从主内存将其读取到本地内存。
- 当处理器写数据时，如果发现操作的是共享变量，会通知其他处理器将该变量的缓存设为无效状态。


这样，其他线程使用缓存时，发现本地工作内存中此变量无效，便从主内存中获取，这样获取到的变量便是最新的值，实现了线程的可见性。


**有序性 :**

为了实现`volatile`的内存语义，编译器在生成字节码时会通过插入内存屏障来禁止指令重排序。

内存屏障：内存屏障是一种CPU指令，它的作用是对该指令前和指令后的一些操作产生一定的约束，保证一些操作按顺序执行。

![](Attachments/Images/Pasted%20image%2020240330130723.png)
注：StoreLoad Barriers同时具备其他三个屏障的作用，它会使得该屏障之前的所有内存访问指令完成之后，才会执行该屏障之后的内存访问命令。

Java内存模型对编译器指定的`volatile`重排序规则为：

- 当第一个操作是`volatile`读时，无论第二个操作是什么都不能进行重排序。
- 当第二个操作是`volatile`写时，无论第一个操作是什么都不能进行重排序。
- 当第一个操作是`volatile`写，第二个操作为`volatile`读时，不能进行重排序。

根据`volatile`重排序规则，Java内存模型采取的是保守的屏障插入策略，`volatile`写是在前面和后面分别插入内存屏障，`volatile`读是在后面插入两个内存屏障，具体如下：

- `volatile`读：在每个`volatile`读后面分别插入LoadLoad屏障及LoadStore屏障（根据`volatile`重排序规则第一条），如下图所示

![在这里插入图片描述](https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/5132bf0f67074c22bebfdd3c2a2f377d~tplv-k3u1fbpfcp-zoom-in-crop-mark:1512:0:0:0.awebp)

​ LoadLoad屏障的作用：禁止上面的所有普通读操作和上面的`volatile`读操作进行重排序。

​ LoadStore屏障的作用：禁止下面的普通写和上面的`volatile`读进行重排序。

- `volatile`写：在每个`volatile`写前面插入一个StoreStore屏障（为满足`volatile`重排序规则第二条），在每个`volatile`写后面插入一个StoreLoad屏障（为满足`volatile`重排序规则第三条），如下图所示

    ![在这里插入图片描述](https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/61481c0662f848588dcce98cea888329~tplv-k3u1fbpfcp-zoom-in-crop-mark:1512:0:0:0.awebp)
    
    StoreStore屏障的作用：禁止上面的普通写和下面的`volatile`写重排序

    StoreLoad屏障的作用：防止上面的`volatile`写与下面可能出现的`volatile`读/写重排序。


## 单例模式

懒汉式 :

```java
public class Singleton {
    private volatile static Singleton uniqueInstance;

    private Singleton() {
    }

    public  static Singleton getUniqueInstance() {
       //先判断对象是否已经实例过，没有实例化过才进入加锁代码
        if (uniqueInstance == null) {
            //类对象加锁
            synchronized (Singleton.class) {
                if (uniqueInstance == null) {
                    uniqueInstance = new Singleton();
                }
            }
        }
        return uniqueInstance;
    }
}
```

饿汉式 : 略

为什么单例模式需要使用volatile修饰 ? 

`uniqueInstance` 采用 `volatile` 关键字修饰也是很有必要的， `uniqueInstance = new Singleton();` 这段代码其实是分为三步执行：

1. 为 `uniqueInstance` 分配内存空间
2. 初始化 `uniqueInstance`
3. 将 `uniqueInstance` 指向分配的内存地址

但是由于 JVM 具有指令重排的特性，执行顺序有可能变成 1->3->2。指令重排在单线程环境下不会出现问题，但是在多线程环境下会导致一个线程获得还没有初始化的实例。例如，线程 T1 执行了 1 和 3，此时 T2 调用 `getUniqueInstance`() 后发现 `uniqueInstance` 不为空，因此返回 `uniqueInstance`，但此时 `uniqueInstance` 还未被初始化

## Cookie和Session的区别

cookie和session都是用来跟踪浏览器用户身份的会话方式

---

Cookie的工作原理

（1）浏览器端第一次发送请求到服务器端  
（2）服务器端创建Cookie，该Cookie中包含用户的信息，然后将该Cookie发送到浏览器端  
（3）浏览器端再次访问服务器端时会携带服务器端创建的Cookie  
（4）服务器端通过Cookie中携带的数据区分不同的用户

![](Attachments/Images/Pasted%20image%2020240409143516.png)

---

Session的工作原理

（1）浏览器端第一次发送请求到服务器端，服务器端创建一个Session，同时会创建一个特殊的Cookie（name为JSESSIONID的固定值，value为session对象的ID），然后将该Cookie发送至浏览器端
（2）浏览器端发送第N（N>1）次请求到服务器端,浏览器端访问服务器端时就会携带该name为JSESSIONID的Cookie对象
（3）服务器端根据name为JSESSIONID的Cookie的value(sessionId),去查询Session对象，从而区分不同用户。
name为JSESSIONID的Cookie不存在（关闭或更换浏览器），返回1中重新去创建Session与特殊的Cookie
name为JSESSIONID的Cookie存在，根据value中的SessionId去寻找session对象
value为SessionId不存在**（Session对象默认存活30分钟）**，返回1中重新去创建Session与特殊的Cookie
value为SessionId存在，返回session对象

![](Attachments/Images/Pasted%20image%2020240409143740.png)

---

区别

session
简单的说，当你登陆一个网站的时候，如果web服务器端使用的是session，那么所有的数据都保存在服务器上，客户端每次请求服务器的时候会发送当前会话sessionid，服务器根据当前sessionid判断相应的用户数据标志，以确定用户是否登陆或具有某种权限。由于数据是存储在服务器上面，所以你不能伪造。

cookie
sessionid是服务器和客户端连接时候随机分配的，如果浏览器使用的是cookie，那么所有数据都保存在浏览器端，比如你登陆以后，服务器设置了cookie用户名，那么当你再次请求服务器的时候，浏览器会将用户名一块发送给服务器，这些变量有一定的特殊标记。服务器会解释为cookie变量，所以只要不关闭浏览器，那么cookie变量一直是有效的，所以能够保证长时间不掉线。

(1)cookie数据存放在客户的浏览器上，session数据放在服务器上
(2)cookie不是很安全，别人可以分析存放在本地的COOKIE并进行COOKIE欺骗,如果主要考虑到安全应当使用session
(3)session会在一定时间内保存在服务器上。当访问增多，会比较占用你服务器的性能，如果主要考虑到减轻服务器性能方面，应当使用COOKIE
(4)单个cookie在客户端的限制是3K，就是说一个站点在客户端存放的COOKIE不能3K。
(5)所以：将登陆信息等重要信息存放为SESSION;其他信息如果需要保留，可以放在COOKIE中

---

Cookie和Session可以在不同的情况下失效：

1. **Cookie的失效时间**：
    
    - 某些cookie可能具有过期时间，即在一定时间后自动失效。这通常由服务器设置，可以是几分钟、几小时、几天或更长时间，具体取决于开发人员的设置。
    - 如果一个cookie没有设置过期时间，那么它会成为会话cookie，并且在用户关闭浏览器时自动失效。
2. **Session的失效**：
    
    - 会话通常在用户关闭浏览器时会结束，这会导致会话中存储的数据失效。
    - 另外，服务器可能会配置会话超时时间，即在用户一段时间内没有活动时会话自动失效。这可以避免不活动会话占用服务器资源。
    - 如果用户手动注销或者通过应用程序进行注销操作，会话也会失效。

需要注意的是，即使会话失效，仍然可以通过设置新的会话来创建新的cookie，但之前的会话数据将不再可用。因此，失效时间的设置对于平衡用户体验和安全性非常重要。

## 为什么Token可以防止CSRF攻击

**CSRF(Cross Site Request Forgery)** 一般被翻译为 **跨站请求伪造** 。那么什么是 **跨站请求伪造** 呢？说简单点，就是用你的身份去发送一些对你不友好的请求。

`Session` 认证中 `Cookie` 中的 `SessionId` 是由浏览器发送到服务端的，借助这个特性，攻击者就可以通过让用户误点攻击链接，达到攻击效果。

但是，我们使用 `Token` 的话就不会存在这个问题，在我们登录成功获得 `Token` 之后，一般会选择存放在 `localStorage` （浏览器本地存储）中。然后我们在前端通过某些方式会给每个发到后端的请求加上这个 `Token`,这样就不会出现 CSRF 漏洞的问题。因为，即使你点击了非法链接发送了请求到服务端，这个非法请求是不会携带 `Token` 的，所以这个请求将是非法的。

其实就是cookie会自动加上 , 但是token是手动加的

## JWT

JWT （JSON Web Token） 是目前最流行的跨域认证解决方案，是一种基于 Token 的认证授权机制。 从 JWT 的全称可以看出，JWT 本身也是 Token，一种规范化之后的 JSON 结构的 Token。

JWT 自身包含了身份验证所需要的所有信息，因此，我们的服务器不需要存储 Session 信息。这显然增加了系统的可用性和伸缩性，大大减轻了服务端的压力。

可以看出，**JWT 更符合设计 RESTful API 时的「Stateless（无状态）」原则** 。

并且， 使用 JWT 认证可以有效避免 CSRF 攻击，因为 JWT 一般是存在在 localStorage 中，使用 JWT 进行身份验证的过程中是不会涉及到 Cookie 的。

JWT 本质上就是一组字串，通过（`.`）切分成三个为 Base64 编码的部分：

- **Header** : 描述 JWT 的元数据，定义了生成签名的算法以及 `Token` 的类型。
- **Payload** : 用来存放实际需要传递的数据
- **Signature（签名）**：服务器通过 Payload、Header 和一个密钥(Secret)使用 Header 里面指定的签名算法（默认是 HMAC SHA256）生成。

  
## 跨域

我们先了解下【域】的定义：**协议 +域名+ 端口**。三者完全相同则为同域，反之有其一不同均为不同域。那么，什么是跨域请求？当前【发起请求】的域和【请求指向】的域属于不同域时，该次请求称之为跨域请求

在 Spring Boot 中跨域问题有很多种解决方案，比如以下 5 个：  
使用 @CrossOrigin 注解实现跨域；  
通过配置文件实现跨域 , 重写addCorsMapping , 放行需要跨域的接口
通过 CorsFilter 对象实现跨域；  

```java
@Configuration
public class CorsConfig {
    // 跨域请求处理
    @Bean
    public CorsFilter corsFilter() {
        CorsConfiguration config = new CorsConfiguration();
        //允许所有域名进行跨域调用
        config.addAllowedOrigin("*");
        //允许所有请求头
        config.addAllowedHeader("*");
        //允许所有方法
        config.addAllowedMethod("*");
        UrlBasedCorsConfigurationSource source = new UrlBasedCorsConfigurationSource();
        source.registerCorsConfiguration("/**", config);
        return new CorsFilter(source);
    }
}

@Configuration
public class CorsConfiguration implements WebMvcConfigurer{

    @Override
    public void addCorsMappings(CorsRegistry registry) {
        registry.addMapping("/**")
                .allowedOriginPatterns("*")
                .allowCredentials(true)
                .allowedMethods("GET", "POST", "DELETE", "PUT")
                .maxAge(3600);
    }
}
```

## JVM死锁如何排查

可以先jps找出所有的java进程

![](Attachments/Images/Pasted%20image%2020240409233114.png)

然后jstack \<pid> 就可以看到里面线程的信息 , 它会表明线程的状态 , 可以自动查找出死锁的线程

![](Attachments/Images/Pasted%20image%2020240409233335.png)

![](Attachments/Images/Pasted%20image%2020240409233354.png)

---

还可以使用jconsole来可视化分析
jconsole可以查看当前进程的内存使用情况 , gc状况 , cpu占用率 , 还可以查看线程的状态 , 还提供了一键检测死锁的功能

![](Attachments/Images/Pasted%20image%2020240409233642.png)

![](Attachments/Images/Pasted%20image%2020240409233710.png)

## OOM怎么排查

为什么会出现 OOM，一般由这些问题引起

1. 分配过少：JVM 初始化内存小，业务使用了大量内存；或者不同 JVM 区域分配内存不合理
2. 代码漏洞：某一个对象被频繁申请，不用了之后却没有被释放，导致内存耗尽

比较常见的 OOM 类型有以下几种

**java.lang.OutOfMemoryError: PermGen space**

Java7 永久代（方法区）溢出，它用于存储已被虚拟机加载的类信息、常量、静态变量、即时编译器编译后的代码等数据。每当一个类初次加载的时候，元数据都会存放到永久代

一般出现于大量 Class 对象或者 JSP 页面，或者采用 CgLib 动态代理技术导致

我们可以通过 `-XX：PermSize` 和 `-XX：MaxPermSize` 修改方法区大小

> Java8 将永久代变更为元空间，报错：java.lang.OutOfMemoryError: Metadata space，元空间内存不足默认进行动态扩展

**java.lang.StackOverflowError**

**虚拟机栈溢出**，一般是由于程序中存在 **死循环或者深度递归调用** 造成的。如果栈大小设置过小也会出现溢出，可以通过 `-Xss` 设置栈的大小

虚拟机抛出栈溢出错误，可以在日志中定位到错误的类、方法

**java.lang.OutOfMemoryError: Java heap space**

**Java 堆内存溢出**，溢出的原因一般由于 JVM 堆内存设置不合理或者内存泄漏导致

如果是内存泄漏，可以通过工具查看泄漏对象到 GC Roots 的引用链。掌握了泄漏对象的类型信息以及 GC Roots 引用链信息，就可以精准地定位出泄漏代码的位置

如果不存在内存泄漏，就是内存中的对象确实都还必须存活着，那就应该检查虚拟机的堆参数（-Xmx 与 -Xms），查看是否可以将虚拟机的内存调大些

---

可以使用jmap来进行分析

jmap -heap \<pid> 可以查看当前的堆内存信息 , 

![](Attachments/Images/Pasted%20image%2020240410000154.png)

jmap -histo:live \<pid> | more 可以分析出大对象的内存占用 , `-histo` 选项是 `jmap` 命令的一个参数，用于生成 Java 进程的对象直方图（Histogram）

![](Attachments/Images/Pasted%20image%2020240410000408.png)

还可以进行Dump文件分析

Dump 文件是 Java 进程的内存镜像，其中主要包括 **系统信息**、**虚拟机属性**、**完整的线程 Dump**、**所有类和对象的状态** 等信息

当程序发生内存溢出或 GC 异常情况时，怀疑 JVM 发生了 **内存泄漏**，这时我们就可以导出 Dump 文件分析

JVM 启动参数配置添加以下参数

- -XX:+HeapDumpOnOutOfMemoryError
- -XX:HeapDumpPath=./（参数为 Dump 文件生成路径）

> 当 JVM 发生 OOM 异常自动导出 Dump 文件，文件名称默认格式：`java_pid{pid}.hprof`

上面配置是在应用抛出 OOM 后自动导出 Dump，或者可以在 JVM 运行时导出 Dump 文件 
使用 jmap -dump:file=\[文件路径] \[pid] 就可以到处dump快照文件

生成了快照文件之后 , 可以使用JvisualVM来进行分析

![](Attachments/Images/Pasted%20image%2020240410002014.png)

---

总结 : 

线上如遇到 JVM 内存溢出，可以分以下几步排查

1. `jmap -heap` 查看是否内存分配过小
2. `jmap -histo` 查看是否有明显的对象分配过多且没有释放情况
3. `jmap -dump` 导出 JVM 当前内存快照，使用 JDK 自带或 MAT 等工具分析快照


## 接口幂等


## git

## 十大排序

![](Attachments/Images/Pasted%20image%2020240408133823.png)

### 冒泡 , 选择与插入

**冒泡排序**

```java
public static void sort(int[] arr) {  
    int len = arr.length;  
    for (int i = 1; i <= len - 1; i++) {  
        for (int j = 0; j <= len - i - 1; j++) {  
            if (arr[j] > arr[j + 1]) {  
                swap(arr, j, j + 1);  
            }  
        }  
    }  
}
```

---

**选择排序**

```java
public static void sort(int[] arr) {  
    int len = arr.length;  
    for (int i = 0; i < len; i++) {  
        int index = i;  
        for (int j = i + 1; j < len; j++) {  
            if(arr[j]<arr[index]){  
                index = j;  
            }  
        }  
        swap(arr,i,index);  
    }  
}
```

---

**插入排序**

```java
public static void sort(int[] arr) {  
    int len = arr.length;  
    for (int i = 1; i < len; i++) {  
        int j = i - 1;  
        int cur = arr[i];  
        while (j >= 0 && arr[j] > cur) {  
            arr[j + 1] = arr[j];  
            j--;  
        }  
        arr[j + 1] = cur;  
    }  
}
```

---

### 希尔排序

![](Attachments/Images/Pasted%20image%2020240408141251.png)

```java
public static void sort(int[] arr) {  
    int len = arr.length;  
    int gap = len / 2;  
    while (gap > 0) {  
        for (int i = gap; i < len; i++) {  
            int cur = arr[i];  
            int j = i - gap;  
            while (j >= 0 && arr[j] > cur) {  
                arr[j + gap] = arr[j];  
                j -= gap;  
            }  
            arr[j + gap] = cur;  
        }  
        gap /= 2;  
    }  
}
```

---

### 归并排序

```java
public static int[] sort(int[] arr) {  
    if (arr.length <= 1) {  
        return arr;  
    }  
    int l = 0, r = arr.length - 1;  
    int mid = (l + r) / 2;  
    int[] left = Arrays.copyOfRange(arr, l, mid + 1);  
    int[] right = Arrays.copyOfRange(arr, mid + 1, r + 1);  
    return merge(sort(left), sort(right));  
}  
  
public static int[] merge(int[] arr, int[] brr) {  
    int lena = arr.length, lenb = brr.length;  
    int[] res = new int[lena + lenb];  
    int i = 0, j = 0;  
    int cnt = 0;  
    while (i < lena && j < lenb) {  
        if (arr[i] < brr[j]) {  
            res[cnt++] = arr[i];  
            i++;  
        } else {  
            res[cnt++] = brr[j];  
            j++;  
        }  
    }  
    if (j < lenb) {  
        i = j;  
        arr = brr;  
        lena = lenb;  
    }  
    while (i < lena) {  
        res[cnt++] = arr[i++];  
    }  
    return res;  
}
```

---

### 快速排序

```java
public static void sort(int[] arr) {  
    partition(arr, 0, arr.length - 1);  
}  
  
public static void partition(int[] arr, int l, int r) {  
    if (l >= r) {  
        return;  
    }  
    int ll = l - 1, rr = r + 1;  
    int i = l;  
    int cur = arr[l];  
    while (i < rr) {  
        if (arr[i] < cur) {  
            swap(arr, i++, ++ll);  
        } else if (arr[i] > cur) {  
            swap(arr, i, --rr);  
        } else {  
            i++;  
        }  
    }  
    partition(arr, l, i - 1);  
    partition(arr, i + 1, r);  
}
```

---

### 堆排序

构建一个大根堆 , 然后每次都把最大的换到末尾

```java
public static void buildMax(int[] arr) {  
    for (int i = arr.length - 1; i >= 0; i--) {  
        heapify(arr, i);  
    }  
}  
  
static int heapLen;  
  
public static void heapify(int[] arr, int i) {  
    int l = i * 2 + 1, r = i * 2 + 2;  
    int largest = i;  
    if (l < heapLen && arr[l] > arr[largest]) {  
        largest = l;  
    }  
    if (r < heapLen && arr[r] > arr[largest]) {  
        largest = r;  
    }  
    if (largest != i) {  
        swap(arr, i, largest);  
        heapify(arr, largest);  
    }  
}  
  
public static void sort(int[] arr) {  
    heapLen = arr.length;  
    buildMax(arr);  
    for (int i = arr.length - 1; i > 0; i--) {  
        swap(arr, 0, i);  
        heapLen--;  
        heapify(arr, 0);  
    }  
}
```

### 计数排序

计数排序的核心在于将输入的数据值转化为键存储在额外开辟的数组空间中。 作为一种线性时间复杂度的排序，**计数排序要求输入的数据必须是有确定范围的整数**。

计数排序 (Counting sort) 是一种稳定的排序算法。计数排序使用一个额外的数组 `C`，其中第 `i` 个元素是待排序数组 `A` 中值等于 `i` 的元素的个数。然后根据数组 `C` 来将 `A` 中的元素排到正确的位置。**它只能对整数进行排序**

```java
private static int[] getMinAndMax(int[] arr) {  
    int maxValue = arr[0];  
    int minValue = arr[0];  
    for (int i = 0; i < arr.length; i++) {  
        if (arr[i] > maxValue) {  
            maxValue = arr[i];  
        } else if (arr[i] < minValue) {  
            minValue = arr[i];  
        }  
    }  
    return new int[]{minValue, maxValue};  
}  

public static int[] sort(int[] arr) {  
    if (arr.length < 2) {  
        return arr;  
    }  
    int[] extremum = getMinAndMax(arr);  
    int minValue = extremum[0];  
    int maxValue = extremum[1];  
    int[] countArr = new int[maxValue - minValue + 1];  
    int[] result = new int[arr.length];  
  
    for (int i = 0; i < arr.length; i++) {  
        countArr[arr[i] - minValue]++;  
    }  
    for (int i = 1; i < countArr.length; i++) {  
        countArr[i] += countArr[i - 1];  
    }  
    for (int i = 0; i < arr.length; i++) {  
        int index = countArr[arr[i] - minValue] - 1;  
        result[index] = arr[i];  
        countArr[arr[i] - minValue]--;  
    }  
    return result;  
}
```

### 桶排序

桶排序是计数排序的升级版。它利用了函数的映射关系，高效与否的关键就在于这个映射函数的确定。为了使桶排序更加高效，我们需要做到这两点：

1. 在额外空间充足的情况下，尽量增大桶的数量
2. 使用的映射函数能够将输入的 N 个数据均匀的分配到 K 个桶中

桶排序的工作的原理：假设输入数据服从均匀分布，将数据分到有限数量的桶里，每个桶再分别排序（有可能再使用别的排序算法或是以递归方式继续使用桶排序进行

![](Attachments/Images/Pasted%20image%2020240408165704.png)
```java
public static void sort(int[] arr) {  
    List[] buckets = new ArrayList[5];  
    for (int i = 0; i < buckets.length; i++) {  
        buckets[i] = new ArrayList<Integer>();  
    }  
    for (int i = 0; i < arr.length; i++) {  
        int index = arr[i] / 10;//对应的桶号  
        buckets[index].add(arr[i]);  
    }  
    for (int i = 0; i < buckets.length; i++) {  
        buckets[i].sort(null);  
    }  
}
```
### 基数排序

基数排序也是非比较的排序算法，对元素中的每一位数字进行排序，从最低位开始排序，复杂度为 O(n×k)，n 为数组长度，k 为数组中元素的最大的位数；

基数排序是按照低位先排序，然后收集；再按照高位排序，然后再收集；依次类推，直到最高位。有时候有些属性是有优先级顺序的，先按低优先级排序，再按高优先级排序。最后的次序就是高优先级高的在前，高优先级相同的低优先级高的在前。基数排序基于分别排序，分别收集，所以是稳定的。

```java
public static void sort(int[] arr) {  
    int ma = -1;  
    for (int j : arr) {  
        ma = Math.max(ma, j);  
    }  
    int N = 0;  
    while (ma != 0) {  
        N++;  
        ma /= 10;  
    }  
    for (int i = 0; i < N; i++) {  
        List<List<Integer>> radix = new ArrayList<>();  
        for (int j = 0; j < 10; j++) {  
            radix.add(new ArrayList<>());  
        }  
        for (int j : arr) {  
            int index = j / (int) (Math.pow(10, i)) % 10;  
            radix.get(index).add(j);  
        }  
        int cnt = 0;  
        for (List<Integer> list : radix) {  
            for (int j : list) {  
                arr[cnt++] = j;  
            }  
        }  
    }  
}
```

## 缓存与数据库一致性问题

### 先写缓存再写数据库

![[Attachments/Pasted image 20240414135517.png]]

### 先写数据库再写缓存

![[Attachments/Pasted image 20240414135605.png]]

### 先删除缓存再写数据库

假设有两个并发的读写操作，一个是写操作，另一个是读操作。

1. 并发读写的情况下，写操作首先删除缓存，接下来需要执行更新数据库操作。
2. 读操作发生，由于缓存已经被删除，读操作不得不从数据库中读取数据。然而，由于写操作尚未完成，数据库中的数据仍然是过时的。
3. 写操作这时需要更新数据库中的值，更新后 MySQL 数据库是最新的值。
4. 读操作将从数据库中查询到的过时数据再回写到缓存。

在这种情况下，读操作获取到的是过时的数据，尽管写操作已经完成。因为缓存被删除，读操作不得不从数据库中读取旧值，而不是最新的值。

![[Attachments/Pasted image 20240414135843.png]]

### 缓存双删

如果说上图的读请求回写缓存在写请求第二次删除缓存之前，那这种技术方案是比较好的，而且也不用引入过多复杂的中间件。

问题就在于，第二次删除缓存，不一定在读请求回写缓存之后。所以我们需要保证第二次删除要在请求回写缓存之后。

假设读请求回写缓存大概需要 300ms，那我们是否可以在写请求第二次删除缓存前进行一个延迟操作，比如睡眠 500ms 后再删除？这样就可以规避读请求回写缓存在第二次删除之后了。

这种方案理论上是可以的，不过把这个睡眠操作使用延迟队列或者引入三方消息队列去做。

![[Attachments/Pasted image 20240414140019.png]]

如果消息队列更新缓存失败了呢？其实这一点还好，凭借消息队列客户端消费的重试规则，如果更新失败次数都达到客户端重试阈值还是不行，那一定是数据或者缓存中间件有问题。

当然，如果重试次数多了，也必然会面临缓存与数据库不一致的时间变长了，这个是需要清楚的。

通过该技术方案，可以很好达到缓存与数据库最终一致性。

### 先写数据库再删除缓存

**第一个小问题**

读请求第一次查询时，会查询到一个错误的数据，因为写请求还没有更新到缓存，写请求写入 MySQL 成功后会删除缓存中的历史数据。后续读请求查询缓存没有值就会再请求数据库 MySQL 进行重新加载，并将正确的值放到缓存中。

也就是说这种模型会存在一个很小周期的缓存与数据库不一致的情况，不过对于绝大多数的情况来说，是可以容忍的。除去一些电商库存、列车余票等对数据比较敏感的情况，比较适合绝大多数业务场景。

![[Attachments/Pasted image 20240414141030.png]]

**第二个小问题**

读的时候读缓存失效了

![[Attachments/Pasted image 20240414141107.png]]

当缓存过期（可能是缓存正常过期也可能是 Redis 内存满了触发清理策略）条件满足，同时读请求的回写缓存 Redis 的执行周期在数据库删除之前，那么就有可能触发缓存数据库不一致问题。

上面说的两种情况，缺一不可，不过能同时满足这两种情况概率极低，低到可以忽略这种情况。

### Canal

![[Attachments/Pasted image 20240414141433.png]]

**存在的第一个问题**

如果是扣减库存的方案，比如说你将列车余票扣减为 16，但是同时又有一个请求将列车余票扣减为 15，这个时候，扣减为 15 的这个请求先到消息队列执行，将缓存更新为余票 15，但是随之而来的是第一个请求余票为 16，会将缓存余票为 15 给覆盖掉。

类似于这种逻辑，会存在一些数据一致性的问题

针对这种问题背景，我们可以提出两种解决方案，同时对其进行优化和补充说明：

1. **顺序消息队列解决方案**：针对那些不经常变更的数据，可以使用消息队列来保证修改变更的顺序性。通过将每次修改操作作为一个顺序消息发送到消息队列中，可以确保消息按照发送的顺序被处理，从而避免了ABA问题的发生。然而，需要注意的是，顺序消息的解决方案也存在一定的风险。如果某个列车数据异常导致消息阻塞，可能会影响整个消息队列的处理速度和稳定性。
2. **增加版本号解决方案**：在进行修改操作时，先判断当前版本号是否小于要修改的版本号，只有在当前版本号小于目标版本号的情况下才进行修改。通过增加版本号，可以有效避免并发修改引起的数据不一致问题。然而，这种方案需要对现有的数据库和缓存结构进行改动，可能会带来一定的执行成本和复杂性。

**存在的第二个问题**

另外，如果在写入数据库余票 16 前，同时有个查询请求，也会存在数据库不一致问题。比如在写入数据库余票 16 前，将数据库余票 17 获取到，然后等消息队列更新到缓存余票 16 后，再将数据库余票 17 更新到缓存。

这种出问题的概率比较小，因为跨的周期太长了。也是类似于存在一个很小周期的数据不一致性。

## 订单延时关闭选型问题

## 幂等

**什么是幂等问题?**

先说下什么是幂等，幂等性是数学和计算机科学中的概念，用于描述操作无论执行多少次，都产生相同结果的特性。在软件行业中，广泛应用该概念。当我们说一个接口支持幂等性时，无论调用多少次，系统的结果都保持一致。

一般我们在系统中，幂等可能存在两种类型的问题：

- 接口幂等：常说的接口防重复提交。
- 消息队列幂等：如何保障消息队列客户端对相同的消息仅消费一次。

如果不做防重复提交或者幂等，可能会导致以下问题：

1. 数据重复处理：如果用户在某个操作还在处理中时重复提交请求，可能会导致相同的数据被处理多次，造成数据的重复操作和处理逻辑的错误。
2. 重复写入：在某些场景下，请求可能触发对数据库或其他存储系统的写入操作。如果请求被重复提交，可能会导致相同的数据被重复写入，破坏数据的一致性。
3. 资源浪费：重复提交请求可能会导致服务器资源的浪费。如果请求处理的时间较长，重复提交会占用服务器的处理能力，增加服务器的负载，降低系统的性能和吞吐量。
4. 业务逻辑错误：在某些业务场景下，重复提交可能会导致业务逻辑错误。例如，如果用户在生成订单的过程中重复提交订单请求，可能会导致生成多个相同的订单，引发订单的混乱和错误。

### 分布式锁

当用户提交请求时，服务器端可以生成一个唯一的标识，例如使用 UUID。

在处理用户请求之前，服务器尝试获取一个分布式锁。如果成功获取到分布式锁，那么则执行接下来的正常业务逻辑流程。因为锁已经被获取，这样可以确保其他请求无法使用相同的标识，避免重复处理。在请求处理完成后，服务器需要释放分布式锁。

![[Attachments/Pasted image 20240414165204.png]]

### Token令牌

为了防止重复操作，客户端在第一次调用业务请求之前会发送一个获取 Token 的请求。服务端会生成一个全局唯一的 ID 作为 Token，并将其保存在 Redis 中，同时将该 ID 返回给客户端。

在客户端进行第二次业务请求时，必须携带这个 Token。

服务端会验证这个 Token，如果验证成功，则执行业务逻辑并从 Redis 中删除该 Token。

如果验证失败，说明 Redis 中已经没有对应的 Token，表示重复操作，服务端会直接返回指定的结果给客户端。

![[Attachments/Pasted image 20240414165421.png]]

### 去重表

去重表是指在使用 Redis 或者 MySQL 作为存储时，为了实现幂等性而用于记录已经处理过的请求或操作，以防止重复执行。大部分场景下，大家会使用 Redis 作为去重组件实现。

去重表只是一个说法，存储到 Redis 的话，其实就是一个 String 的 Key 而已。

具体来说，当客户端发送请求时，服务端会先查询 Redis 去重表来检查该请求是否已经被处理过。如果在存在对应的记录，表示请求已经执行过，服务端可以直接返回之，而不再执行重复操作。如果在不存在对应的记录，表示请求是新的，服务端会执行相应的业务逻辑，并在处理完成后将请求的唯一标识（如请求 ID 或标识）添加到 Redis 去重表中，以便后续的重复请求可以被正确识别和处理。

另外，如果消息已经在消费中，抛出异常，消息会触发延迟消费，在消息队列消费失败的场景下即发送到重试队列 `RETRY TOPIC`。

![[Attachments/Pasted image 20240414170247.png]]

### 幂等组件如何设计

1. **自定义一个幂等注解 , 里面融合了上面三种通用解决幂等的所有属性**

![[Attachments/Pasted image 20240414172840.png]]

```java
/**
 * 幂等注解
 */
@Target({ElementType.TYPE, ElementType.METHOD})
@Retention(RetentionPolicy.RUNTIME)
@Documented
public @interface Idempotent {

    /**
     * 幂等Key，只有在 {@link Idempotent#type()} 为 {@link IdempotentTypeEnum#SPEL} 时生效
     */
    String key() default "";

    /**
     * 触发幂等失败逻辑时，返回的错误提示信息
     */
    String message() default "您操作太快，请稍后再试";

    /**
     * 验证幂等场景，支持多种 {@link IdempotentSceneEnum}
     */
    IdempotentSceneEnum scene() default IdempotentSceneEnum.RESTAPI;

    /**
     * 验证幂等类型，支持多种幂等方式
     * RestAPI 建议使用 {@link IdempotentTypeEnum#TOKEN} 或 {@link IdempotentTypeEnum#PARAM}
     * 其它类型幂等验证，使用 {@link IdempotentTypeEnum#SPEL}
     */
    IdempotentTypeEnum type() default IdempotentTypeEnum.PARAM;

    /**
     * 设置防重令牌 Key 前缀，MQ 幂等去重可选设置
     * {@link IdempotentSceneEnum#MQ} and {@link IdempotentTypeEnum#SPEL} 时生效
     */
    String uniqueKeyPrefix() default "";

    /**
     * 设置防重令牌 Key 过期时间，单位秒，默认 1 小时，MQ 幂等去重可选设置
     * {@link IdempotentSceneEnum#MQ} and {@link IdempotentTypeEnum#SPEL} 时生效
     */
    long keyTimeout() default 3600L;
}

```

重要的字段有两个，`scene` 和 `type`。先说场景 `scene` 字段，该字段代表了是接口的防重复提交还是消息队列的防重复消费，通过一个枚举标识。

```java
public enum IdempotentSceneEnum {
    
    /**
     * 基于 RestAPI 场景验证
     */
    RESTAPI,
    
    /**
     * 基于 MQ 场景验证
     */
    MQ
}
```

然后就是 type 字段，记录了使用什么方式实现幂等，其中 TOKEN 和 PARAM 以及 SPEL 都可以应用于接口防重复提交，SPEL 应用于消息队列防重复消费。

然后从分布式锁、Token 令牌以及去重表实现上来说，有个对应关系：

- 分布式锁：PARAM 和 SPEL。
- Token 令牌：TOKEN。
- 去重表：SPEL。

```java
/**
 * 幂等验证类型枚举
 */
public enum IdempotentTypeEnum {
    
    /**
     * 基于 Token 方式验证
     */
    TOKEN,
    
    /**
     * 基于方法参数方式验证
     */
    PARAM,
    
    /**
     * 基于 SpEL 表达式方式验证
     */
    SPEL
}
```

---

2. **幂等AOP**

我们使用 AOP 技术为方法增强提供了通用的幂等性保证，只需要在需要保证幂等性的方法上添加 `@Idempotent` 注解，`Aspect` 就会对该方法进行增强。

简单来说，就是先获取到方法上的幂等注解，然后获取到对应的幂等处理实现类。通过实现类进行幂等前置逻辑，执行完后操作具体被注解修饰的方法，最终执行释放资源的后置逻辑

```java
/**
 * 幂等注解 AOP 拦截器
 */
@Aspect
public final class IdempotentAspect {

    /**
     * 增强方法标记 {@link Idempotent} 注解逻辑
     */
    @Around("@annotation(org.opengoofy.index12306.framework.starter.idempotent.annotation.Idempotent)")
    public Object idempotentHandler(ProceedingJoinPoint joinPoint) throws Throwable {
        // 获取到方法上的幂等注解实际数据
        Idempotent idempotent = getIdempotent(joinPoint);
        // 通过幂等场景以及幂等类型，获取幂等执行处理器
        IdempotentExecuteHandler instance = IdempotentExecuteHandlerFactory.getInstance(idempotent.scene(), idempotent.type());
        Object resultObj;
        try {
            // 执行幂等处理逻辑
            instance.execute(joinPoint, idempotent);
            // 如果幂等处理逻辑没有抛异常，处理中间业务
            resultObj = joinPoint.proceed();
            // 处理幂等后置逻辑，比如释放资源或者锁之类的
            instance.postProcessing();
        } catch (RepeatConsumptionException ex) {
            /**
             * 该异常为消息队列防重复提交独有，触发幂等逻辑时可能有两种情况：
             *    * 1. 消息还在处理，但是不确定是否执行成功，那么需要返回错误，方便 RocketMQ 再次通过重试队列投递
             *    * 2. 消息处理成功了，该消息直接返回成功即可
             */
            if (!ex.getError()) {
                return null;
            }
            throw ex;
        } catch (Throwable ex) {
            // 客户端消费存在异常，需要删除幂等标识方便下次 RocketMQ 再次通过重试队列投递
            instance.exceptionProcessing();
            throw ex;
        } finally {
            // 清理幂等容器上下文
            IdempotentContext.clean();
        }
        return resultObj;
    }

    public static Idempotent getIdempotent(ProceedingJoinPoint joinPoint) throws NoSuchMethodException {
        MethodSignature methodSignature = (MethodSignature) joinPoint.getSignature();
        Method targetMethod = joinPoint.getTarget().getClass().getDeclaredMethod(methodSignature.getName(), methodSignature.getMethod().getParameterTypes());
        return targetMethod.getAnnotation(Idempotent.class);
    }
}
```

可以看到 , 实际的执行幂等逻辑的代码是instance.execute(joinPoint, idempotent); 这里的instance是IdempotentExecuteHandler instance = IdempotentExecuteHandlerFactory.getInstance(idempotent.scene(), idempotent.type());

也就是说是通过scene和type两个字段来确定使用的幂等逻辑的


## CAP理论

CAP原则又称CAP定理，指的是在一个分布式系统中，Consistency（一致性）、 Availability（可用性）、Partition tolerance（分区容错性）这3个基本需求，最多只能同时满足其中的2个。

![[Attachments/Pasted image 20240413232800.png]]

**为什么CAP不可兼得?**

首先对于分布式系统，分区是必然存在的，所谓分区指的是分布式系统可能出现的字区域网络不通，成为孤立区域的的情况。

那么满足分区容错的基础上，能不能同时满足`一致性`和`可用性`？

假如现在有两个分区`N1`和`N2`，N1和N2分别有不同的分区存储D1和D2，以及不同的服务S1和S2。

- 在满足`一致性` 的时候，N1和N2的数据要求值一样的，D1=D2。
- 在满足`可用性`的时候，无论访问N1还是N2，都能获取及时的响应。

假如现在有这样的场景：

- 用户访问了N1，修改了D1的数据。
- 用户再次访问，请求落在了N2。此时D1和D2的数据不一致。

接下来：

- 保证`一致性`：此时D1和D2数据不一致，要保证一致性就不能返回不一致的数据，`可用性`无法保证。
- 保证`可用性`：立即响应，可用性得到了保证，但是此时响应的数据和D1不一致，`一致性`无法保证。

所以，可以看出，分区容错的前提下，`一致性`和`可用性`是矛盾的。

## 雪花算法

**雪花算法组成**

![[Attachments/Pasted image 20240414130713.png]]

包含四个组成部分：

**不使用**：1bit，最高位是符号位，0 表示正，1 表示负，固定为 0。

**时间戳**：41bit，毫秒级的时间戳（41 位的长度可以使用 69 年）。

**标识位**：5bit 数据中心 ID，5bit 工作机器 ID，两个标识位组合起来最多可以支持部署 1024 个节点。

**序列号**：12bit 递增序列号，表示节点毫秒内生成重复，通过序列号表示唯一，12bit 每毫秒可产生 4096 个 ID。

通过序列号 1 毫秒可以产生 4096 个不重复 ID，则 1 秒可以生成 4096 * 1000 = 409w ID。

默认的雪花算法是 64 bit，具体的长度可以自行配置。如果希望运行更久，**增加时间戳的位数**；如果需要支持更多节点部署，**增加标识位长度**；如果并发很高，**增加序列号位数。**

**总结**：雪花算法并不是一成不变的，可以根据系统内具体场景进行定制。

---

**SnowFlake 算法的优点：**

1. 高性能高可用：生成时不依赖于数据库，完全在内存中生成。
2. 高吞吐：每秒钟能生成数百万的自增 ID。
3. ID 自增：存入数据库中，索引效率高。

**SnowFlake 算法的缺点：**

依赖与系统时间的一致性，如果系统时间被回调，或者改变，可能会造成 ID 冲突或者重复。

---

**雪花算法生成ID重复的问题**

**假设**：一个订单微服务，通过雪花算法生成 ID，共部署三个节点，标识位一致。

此时有 200 并发，均匀散布三个节点，三个节点同一毫秒同一序列号下生成 ID，那么就会产生重复 ID。

通过上述假设场景，可以知道雪花算法生成 ID 冲突存在一定的前提条件：

1. 服务通过集群的方式部署，其中部分机器标识位一致。
2. 业务存在一定的并发量，没有并发量无法触发重复问题。
3. 生成 ID 的时机：同一毫秒下的序列号一致。

---

**标识位如何定义**

如果能保证标识位不重复，那么雪花 ID 也不会重复。

通过上面的案例，知道了 ID 重复的必要条件。如果要避免服务内产生重复的 ID，那么就需要从标识位上动文章。

我们先看看开源框架中使用雪花算法，如何定义标识位。

Mybatis-Plus v3.4.2 雪花算法实现类 Sequence，提供了两种构造方法：无参构造，自动生成 dataCenterId 和 workerId；有参构造，创建 Sequence 时明确指定标识位。

Hutool v5.7.9 参照了 Mybatis-Plus dataCenterId 和 workerId 生成方案，提供了默认实现

```java
public static long getDataCenterId(long maxDatacenterId) {
    long id = 1L;
    final byte[] mac = NetUtil.getLocalHardwareAddress();
    if (null != mac) {
        id = ((0x000000FF & (long) mac[mac.length - 2])
                | (0x0000FF00 & (((long) mac[mac.length - 1]) << 8))) >> 6;
        id = id % (maxDatacenterId + 1);
    }

    return id;
}
```

入参 `maxDatacenterId` 是一个固定值，代表数据中心 ID 最大值，默认值 31。

为什么最大值要是 31？因为 5bit 的二进制最大是 11111，对应十进制数值 31。

  

获取 dataCenterId 时存在两种情况，一种是网络接口为空，默认取 1L；另一种不为空，通过 Mac 地址获取 dataCenterId。

可以得知，dataCenterId 的取值与 Mac 地址有关。

接下来再看看 workerId。

```java
public static long getWorkerId(long datacenterId, long maxWorkerId) {
    final StringBuilder mpid = new StringBuilder();
    mpid.append(datacenterId);
    try {
        mpid.append(RuntimeUtil.getPid());
    } catch (UtilException igonre) {
        //ignore
    }
    return (mpid.toString().hashCode() & 0xffff) % (maxWorkerId + 1);
}
```

入参 maxWorkderId 也是一个固定值，代表工作机器 ID 最大值，默认值 31；datacenterId 取自上述的 getDatacenterId 方法。

name 变量值为 `PID@IP`，所以 name 需要根据 `@` 分割并获取下标 0，得到 PID。

通过 MAC + PID 的 hashcode 获取 16 个低位，进行运算，最终得到 workerId。

---

**标识位如何分配**

Mybatis-Plus 标识位的获取依赖 Mac 地址和进程 PID，虽然能做到尽量不重复，但仍有小几率。

标识位如何定义才能不重复？有两种方案：**预分配和动态分配。**

**预分配**

应用上线前，统计当前服务的节点数，人工去申请标识位。

这种方案，没有代码开发量，在服务节点固定或者项目少可以使用，但是解决不了服务节点动态扩容性问题。

**动态分配**

通过将标识位存放在 Redis、Zookeeper、MySQL 等中间件，在服务启动的时候去请求标识位，请求后标识位更新为下一个可用的。

通过存放标识位，延伸出一个问题：雪花算法的 ID 是 **服务内唯一还是全局唯一。**

以 Redis 举例，如果要做服务内唯一，存放标识位的 Redis 节点使用自己项目内的就可以；如果是全局唯一，所有使用雪花算法的应用，要用同一个 Redis 节点。

两者的区别仅是 **不同的服务间是否公用 Redis**。如果没有全局唯一的需求，最好使 ID 服务内唯一，因为这样可以避免单点问题。

服务的节点数超过 1024，则需要做额外的扩展；可以扩展 10 bit 标识位，或者选择开源分布式 ID 框架。

可以使用redis里面的lua脚本来实现

两个key , 分别从0-31循环来分配就可以了

