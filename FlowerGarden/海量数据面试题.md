
## 统计不同号码的个数

已知某个⽂件内包含⼤量的电话号码，每个号码的数字为 8 位，怎么统计不同号码的个数？

8 位的电话号码可以表示的范围为 00000000 ~ 99999999。如果⽤ bit 表示⼀个号码，那么⼀共需要 1 亿个 bit，只需要⼤约 10 MB 的内存。 计算过程如下： 00000000 ~ 99999999 ⼀共有 1 亿个数字， 1 亿 bit = 10 ^ 8 / ( 8 ^ 1000 ^ 1000) = 12.5 MB 这个时候，我们申请⼀个位图并且初始化为 0 ，然后遍历所有的电话号码，把遍历到的电话号码对应的 位图中的 bit 设置为 1 。当遍历完成之后，如果 bit 值为 1，则表示这个电话号码在⽂件中存在，如果 bit 值为 0 则表示这个电话号码在⽂件中不存在。 最后，这个位图中 bit 值为 1 的数量就是不同电话号码的个数了。 那么，如何确定电话号码对应的是位图中的哪⼀位呢？ 可以使⽤下⾯的⽅法来实现电话号码和位图的映射。

第⼀步，因为int整数占⽤4\*8=32bit，通过 P/32 就可以计算出该电话号码在 bitmap 数组中的下标，从 ⽽可以确定它对应的 bit 在数组中的位置。 第⼆步，通过 P%32 就可以计算出这个电话号码在这个int数字中具体的bit的位置。只要把1向左移 P%32 位，然后把得到的值与这个数组中的值做或运算，就可以把这个电话号码在位图中对应的位设置 为1。

![](Attachments/Images/Pasted%20image%2020240411141707.png)

## 出现频率最高的100个词

假如有⼀个 1G ⼤⼩的⽂件，⽂件⾥每⼀⾏是⼀个词，每个词的⼤⼩不超过 16 bytes，要求返回出现频 率最⾼的 100 个词。内存限制是 10M。

**解法一 : (只适用于部分)**

第⼀步，⾸先遍历⼀遍⼤⽂件，对遍历到的每个词 x ，执⾏ hash（x）% 500,将结果为 i 的词存放到⽂ 件 f（i）中，遍历结束之后，可以得到 500 个⼩⽂件，每个⼩⽂件的⼤⼩为 2 M 左右； 
第⼆步，接着统计每个⼩⽂件中出现频率最⾼的 100 个词。可以⽤ HashMap 来实现，其中 key 为词， value 为该次出现的频率
第三步，在第⼆步中找出了每个⽂件出现频率最⾼的 100 个词之后，通过维护⼀个⼩顶堆来找出所有⼩ ⽂件中出现频率最⾼的 100 个词。

总结⼀下，这个解法的主要思路如下：
1. 采⽤分治的思想，进⾏哈希取余 
2. 使⽤ HashMap 统计每个⼩⽂件单词出现的次数 
3. 使⽤⼩顶堆，遍历步骤 2 中的⼩⽂件，找到词频 top 100 的单词。

很容易就会发现⼀个问题，如果第⼆步中，如果这个 1 G 的⼤⽂件中有某个词的频率太⾼，可能导致⼩⽂件⼤⼩超过 10 M，这种情况该怎么处理呢？

---

解法二 : 

多路归并排序对⼤⽂件排序的步骤如下： 
1. 将⽂件按照顺序切分成⼤⼩不超过 2 M 的⼩⽂件，总共 500 个⼩⽂件 
2. 使⽤ 10 MB 内存分别对 500 个⼩⽂件中的单词进⾏排序 
3. 使⽤⼀个⼤⼩为 500 的堆，对 500 个⼩⽂件进⾏多路排序，然后将结果写到⼀个⼤⽂件中

第三步的思路如下 :

1. 初始化⼀个最⼩堆，⼤⼩就是有序⼩⽂件的个数 500。堆中的每个节点存放每个有序⼩⽂件对应的 输⼊流。 
2. 按照每个有序⽂件中的下⼀⾏数据对所有⽂件输⼊流进⾏排序，单词⼩的输⼊⽂件流放在堆顶。 
3. 拿出堆顶的输⼊流，并且将下⼀⾏数据写⼊到最终排序的⽂件中，如果拿出来的输⼊流还有数据的 话，那么就将这个输⼊流再次添加到栈中。否则说明该⽂件输⼊流中没有数据了，那么可以关闭这 个流。 
4. 循环这个过程，直到所有⽂件输⼊流中没有数据为⽌。
5. 然后就得到了一个大文件 , 里面都是排好序的单词 , 然后用小顶堆就好了

## 5亿个数的大文件怎么排序

其实就是上面的排序方法

## 查找两个大文件共同的URL

给定 a、b 两个⽂件，各存放 50 亿个 URL，每个 URL 各占 64B，找出 a、b 两个⽂件共同的 URL。 内存限制是 4G。

可以采⽤分治策略，也就是把⼀个⽂件中的 URL 按照某个特征划分为多个⼩⽂件，使得每个⼩⽂件⼤⼩ 不超过 4G，这样就可以把这个⼩⽂件读到内存中进⾏处理了。 

⾸先遍历⽂件a，对遍历到的 URL 进⾏哈希取余 hash(URL) % 1000，根据计算结果把遍历到的 URL 存 储到 a0, a1,a2, ..., a999，这样每个⼤⼩约为 300MB。使⽤同样的⽅法遍历⽂件 b，把⽂件 b 中的URL 分别存储到⽂件 b0, b1, b2, ..., b999 中。

这样处理过后，所有可能相同的 URL 都在对应的⼩⽂件 中，即 a0 对应 b0, ..., a999 对应 b999，不对应的⼩⽂件不可能有相同的 URL。那么接下来，我们只 需要求出这 1000 对⼩⽂件中相同的 URL 就好了。 

接着遍历 ai( i∈\[0,999])，把 URL 存储到⼀个 HashSet 集合中。然后遍历 bi 中每个 URL，看在 HashSet 集合中是否存在，若存在，说明这就是共同的 URL，可以把这个 URL 保存到⼀个单独的⽂件 中。

## 海量数据寻找中位数

给定100亿个⽆符号的乱序的整数序列，如何求出这100亿个数的中位数（中位数指的是排序后最中间那 个数），内存只有512M

中位数问题可以看做⼀个统计问题，⽽不是排序问题，⽆符号整数⼤⼩为4B，则能表示的数的范围为为 0 ~ 2^32 - 1（40亿），如果没有限制内存⼤⼩，则可以⽤⼀个2^32（4GB）⼤⼩的数组（也叫做桶） 来保存100亿个数中每个⽆符号数出现的次数。遍历这100亿个数，当元素值等于桶元素索引时，桶元素 的值加1。当统计完100亿个数以后，则从索引为0的值开始累加桶的元素值，当累加值等于50亿时，这个 值对应的索引为中位数。时间复杂度为O(n)。

因为题⽬要求内存限制512M，所以上述解法不合适。

下⾯分享另⼀种解法（分治法的思想） 如果100亿个数字保存在⼀个⼤⽂件中，可以依次读⼀部分⽂件到内存(不超过内存限制)，将每个数字⽤ ⼆进制表示，⽐较⼆进制的最⾼位(第32位，符号位，0是正，1是负)。

如果数字的最⾼位为0，则将这个数字写⼊ file_0⽂件中；如果最⾼位为 1，则将该数字写⼊file_1⽂件 中。 从⽽将100亿个数字分成了两个⽂件。

假设 file_0⽂件中有 60亿 个数字，file_1⽂件中有 40亿 个数字。那么中位数就在 file_0 ⽂件中，并且 是 file_0 ⽂件中所有数字排序之后的第 10亿 个数字。因为file_1中的数都是负数，file_0中的数都是正 数，也即这⾥⼀共只有40亿个负数，那么排序之后的第50亿个数⼀定位于file_0中。

现在，我们只需要处理 file_0 ⽂件了，不需要再考虑file_1⽂件。对于 file_0 ⽂件，同样采取上⾯的措施 处理：将file_0⽂件依次读⼀部分到内存(不超过内存限制)，将每个数字⽤⼆进制表示，⽐较⼆进制的 次 ⾼位（第31位），如果数字的次⾼位为0，写⼊file_0_0⽂件中；如果次⾼位为1，写⼊file_0_1⽂件 中。 

现假设 file_0_0⽂件中有30亿个数字，file_0_1中也有30亿个数字，则中位数就是：file_0_0⽂件中的数 字从⼩到⼤排序之后的第10亿个数字。

抛弃file_0_1⽂件，继续对 file_0_0⽂件 根据 次次⾼位(第30位) 划分，假设此次划分的两个⽂件为： file_0_0_0中有5亿个数字，file_0_0_1中有25亿个数字，那么中位数就是 file_0_0_1⽂件中的所有数字 排序之后的 第 5亿 个数。

按照上述思路，直到划分的⽂件可直接加载进内存时，就可以直接对数字进⾏快速排序，找出中位数 了。

## 如何查询最热门的查询串

搜索引擎会通过⽇志⽂件把⽤户每次检索使⽤的所有查询串都记录下来，每个查询床的⻓度不超过 255 字节。 

假设⽬前有 1000w 个记录（这些查询串的重复度⽐较⾼，虽然总数是 1000w，但如果除去重复后，则 不超过 300w 个）。请统计最热⻔的 10 个查询串，要求使⽤的内存不能超过 1G。（⼀个查询串的重复 度越⾼，说明查询它的⽤户越多，也就越热⻔。）

---

**分治法**

划分为多个⼩⽂件，保证单个⼩⽂件中的字符串能被直接加载到内存中处理，然后求出每个⽂件中出现 次数最多的 10 个字符串；最后通过⼀个⼩顶堆统计出所有⽂件中出现最多的 10 个字符串。 

⽅法可⾏，但不是最好，下⾯介绍其他⽅法。

---

**HashMap法**

虽然字符串总数⽐较多，但去重后不超过 300w，因此，可以考虑把所有字符串及出现次数保存在⼀个 HashMap 中，所占⽤的空间为 300w*(255+4)≈777M（其中，4表示整数占⽤的4个字节）。由此可 ⻅，1G 的内存空间完全够⽤。

⾸先，遍历字符串，若不在 map 中，直接存⼊ map，value 记为 1；若在 map 中，则把对应的 value 加 1，这⼀步时间复杂度 O(N)。

接着遍历 map，构建⼀个 10 个元素的⼩顶堆，若遍历到的字符串的出现次数⼤于堆顶字符串的出现次 数，则进⾏替换，并将堆调整为⼩顶堆。

遍历结束后，堆中 10 个字符串就是出现次数最多的字符串。这⼀步时间复杂度 O(Nlog10)。

---

**前缀树法**

在遍历字符串时，在前缀树中查找，如果找到，则把结点中保存的字符串次数加 1，否则为这个字符串构 建新结点，构建完成后把叶⼦结点中字符串的出现次数置为 1。 

最后依然使⽤⼩顶堆来对字符串的出现次数进⾏排序。

## 如何找出排名前500的数

有 1w 个数组，每个数组有 500 个元素，并且有序排列。如何在这 10000\*500 个数中找出前 500 的 数？

可以归并 , 略 , 不推荐

---

对本题⽽⾔，假设数组降序排列，可以采⽤以下⽅法： 

⾸先建⽴⼤顶堆，堆的⼤⼩为数组的个数，即为10000 ，把每个数组最⼤的值存到堆中。 

接着删除堆顶元素，保存到另⼀个⼤⼩为 500 的数组中，然后向⼤顶堆插⼊删除的元素所在数组的下⼀ 个元素。 

重复上⾯的步骤，直到删除完第 500 个元素，也即找出了最⼤的前 500 个数。

## 如何按照查询频率排序

有 10 个⽂件，每个⽂件⼤⼩为 1G，每个⽂件的每⼀⾏存放的都是⽤户的 query，每个⽂件的 query 都 可能重复。要求按照 query 的频度排序。

分治法需要根据数据量⼤⼩以及可⽤内存的⼤⼩来确定问题划分的规模。

对于这道题，可以顺序遍历 10 个⽂件中的 query，通过 Hash 函数 hash(query) % 10 把这些 query 划分到 10 个⼩⽂件中。

之后对每 个⼩⽂件使⽤ HashMap 统计 query 出现次数，根据次数排序并写⼊到零外⼀个单独⽂件中。

接着对所有⽂件按照 query 的次数进⾏排序，这⾥可以使⽤归并排序（由于⽆法把所有 query 都读⼊内 存，因此需要使⽤外排序）。

## 用4KB的内存寻找重复元素

用bit数组 , 同统计不同号码的个数

## 从40亿中产生一个不存在的整数

给定⼀个输⼊⽂件，包含 40 亿 个⾮负整数，请设计⼀个算法，产⽣⼀个不存在该⽂件中的整 数，假设你有 1 GB 的内存来完成这项任务，你会怎么实现，然后再难⼀点，只⽤ 10 MB 的内存 呢？

---

如果内存够可以申请一个位数组

如果内存只有10MB , 那么只用位数组就不行了 , 可以采用分块

40亿个数 需要500MB的空间，那如果只有10MB的空间，⾄少需要50个块才可以

⼀般来说，我们划分都是使⽤2的整数倍，因此划分成64个块是合理的。 ⾸先，将0~4 294 967 295(2^32) 这个范围是可以平均分成 64 个区间的，每个区间是 67 108 864 个数，例如： 第0 区间（0~67 108 863） 第 1 区间（67 108 864~134 217 728） 第 i 区间（67 108 864´I~67 108 864´(i+1)-1）， ……， 第 63 区间（4 227 858 432~4 294 967 295）。

因为⼀共只有 40 亿个数，所以，如果统计落在每⼀个区间上的数有多少，肯定有⾄少⼀个区间上 的计数少于67 108 864。利⽤这⼀点可以找出其中⼀个没出现过的数。具体过程是通过两次遍历 来搞定：

第⼀次遍历，先申请⼀个⻓度为 64 的整型数组 countArr\[0···63]，countArr\[i] ⽤来统计区间 i 上的数有多少。遍历 40 亿个数，根据当前数是多少来决定哪⼀个区间上的计数增加。 例如，如果当前数是 3422 552090，其先对 67 108 864 取模，为 51，所以第 51 区间上的计数 增加

countArr\[51]++，遍历⽹ 40 亿个数之后，遍历 countArr，必然会有某⼀个位置上的值 （countArr[i]） ⼩于 67 108 864，表示第 i 区间上⾄少有⼀个数没出现过。我们肯定会找到⾄少 ⼀个这样的区间。

如果重复数怎么办?你重复了那么对应区间的countArr就增加 , 但是别的区间那就没了 , 所有重复越多其实越好统计

此时使⽤的内存就是countArr 的⼤⼩（64\*4B），是⾮常⼩的。 假设找到第 37 区间上的计数⼩于 67 108 864，那么我们对这40亿个数据进⾏第⼆次遍历：

1. 申请⻓度为 67 108 864 的 bit map，这占⽤⼤约 8MB 的空间，记为 bitArr\[0..67108863]。 
2. 遍历这 40 亿个数，此时的遍历只关注落在第 37 区间上的数，记为 num（num满⾜num/67 108 864\==37），其他区间的数全部忽略。 
3. 如果步骤 2 的 num 在第 37 区间上，将 bitArr\[num - 67108864\*37]的值设置为 1，也就是 只做第 37 区间上的数的 bitArr 映射。 
4. 遍历完 40 亿个数之后，在 bitArr 上必然存在没被设置成 1 的位置，假设第 i 个位置上的值没 设置成 1，那么 67 108 864´37+i 这个数就是⼀个没出现过的数。

## 使用2GB内存在20亿个整数中找出出现次数最多的数

还是hash这些数到小文件里面去 , 然后分文件统计就好了

## 100亿个URL里面找出重复的URL

本题的解法是解决⼤数据问题的⼀种常规⽅法：把⼤⽂件通过哈希函数分配到机器， 或者通过哈 希函数把⼤⽂件拆成⼩⽂件，⼀直进⾏这种划分，直到划分的结果满⾜资源限制的要求。⾸先，要 向⾯试官询问在资源上的限制有哪些，包括内存、计算时间等要求。在明确了限制要求之后，可以 将每条 URL 通过哈希函数分配到若⼲台机器或者拆分成若⼲个⼩⽂件， 这⾥的“若⼲”由具体的 资源限制来计算出精确的数量。

例如，将 100 亿字节的⼤⽂件通过哈希函数分配到 100 台机器上，然后每⼀台机器分别统计分给 ⾃⼰的 URL 中是否有重复的 URL，同时哈希函数的性质决定了同⼀条 URL 不可能分给不同的机 器；或者在单机上将⼤⽂件通过哈希函数拆成 1000 个⼩⽂件，对每⼀个⼩⽂件再利⽤哈希表遍 历，找出重复的 URL；还可以在分给机器或拆完⽂件之后进⾏排序，排序过后再看是否有重复的 URL 出现。总之，牢记⼀点，很多⼤数据问题都离不开分流，要么是⽤哈希函数把⼤⽂件的内容 分配给不同的机器，要么是⽤哈希函数把⼤⽂件拆成⼩⽂件，然后处理每⼀个⼩数量的集合。

## 求出每天热门的100词

某搜索公司⼀天的⽤户搜索词汇是海量的（百亿数据量），请设计⼀种求出每天热⻔ Top 100 词 汇的可⾏办法。

这道题和上⾯那道题⼀样，都是采⽤分流的思路来处理，把包含百亿数据量的词汇⽂件分流道不同 的机器上，具体多少台机器由⾯试官规定或者由更多的限制来决定。对每⼀台机器来说，如果分到 的数据量依然很⼤，⽐如，内存不够或存在其他问题，可以再⽤哈希函数把每台机器的分流⽂件拆 成更⼩的⽂件处理。处理每⼀个⼩⽂件的时候，通过哈希表统计每种词及其词频，哈希表记录建⽴ 完成后，再遍历哈希表，遍历哈希表的过程中使⽤⼤⼩为 100 的⼩根堆来选出每⼀个⼩⽂件的 Top 100（整体未排序的 Top 100）。每⼀个⼩⽂件都有⾃⼰词频的⼩根堆（整体未排序的 Top 100），将⼩根堆⾥的词按照词频排序，就得到了每个⼩⽂件的排序后 Top 100。然后把各个⼩⽂ 件排序后的 Top 100 进⾏外排序或者继续利⽤⼩根堆，就可以选出每台机器上的 Top100。不同 机器之间的 Top 100 再进⾏外排序或者继续利⽤⼩根堆，最终求出整个百亿数据量中的 Top 100。对于 Top K 的问题，除⽤哈希函数分流和⽤哈希表做词频统计之外，还经常⽤堆结构和外 排序的⼿段进⾏处理。

## 40亿个非负整数找到出现两次的数

32 位⽆符号整数的范围是 0～4 294 967 295，现在有 40 亿个⽆符号整数，可以使⽤最多 1GB 的内存，找出所有出现了两次的数。

本题和之前 40 亿个数字寻找重复的元素其实差不多，可以说是他的进阶，这道题⽬把出现次数限 制在两次。

⾸先，可以⽤ bit map 的⽅式来表示数出现的情况。具体地说，是申请⼀个⻓度为4 294 967 295x2 的bit 类型的数组bitArr，⽤ 2 个位置表示⼀个数出现的词频，1B 占⽤ 8 个bit， 所以⻓度 为 4 294 967 295x2 的 bit 类型的数组占⽤ 1GB 空间。

怎么使⽤这个 bitArr 数组呢？遍历这 40 亿个⽆符号数，如果初次遇到 num，就把bitArr\[num\*2 + 1]和 bitArr\[num\*2]设置为 01， 如 果第⼆次遇到 num，就把bitArr\[num\*2+1]和bitArr\[num\*2]设置为 10，如果第三次遇到 num， 就 把bitArr\[num\*2+1]和bitArr\[num\*2]设置为 11。以后再遇到 num，发现此时 bitArr\[num\*2+1]和 bitArr\[num\*2]已经被设置为 11，就不再做任何设置。遍历完成后，再依次遍历 bitArr，如果发现 bitArr\[i\*2+1]和bitArr\[i\*2]设置为 10，那么 i 就是出现了两次的数。

## 对20GB文件进行排序

这⾥给出⼤⼩是20GB，其实⾯试官就在暗示你不要将所有的⽂件都装⼊到内存⾥，因此我们只能 将⽂件划分成⼀些块，每块⼤⼩是xMB，x就是可⽤内存的⼤⼩，例如1GB⼀块，那我们就可以将 ⽂件分为20块。我们先对每块进⾏排序，然后再逐步合并。这时候我们可以使⽤两两归并，也可 以使⽤堆排序策略将其逐步合并成⼀个。

## 超大文本中搜索两个单词的最短距离

有个超⼤⽂本⽂件，内部是很多单词组成的，现在给定两个单词，请你找出这两个单词在这个⽂件 中的最⼩距离，也就是相隔⼏个单词。你有办法在O(n)时间⾥完成搜索操作吗？⽅法的空间复杂度 如何。

用index1和index2来记录一下word1和word2最后一次出现的下标 , 每次更新就好了

进阶问题如果寻找过程在这个⽂件中会重复多次，⽽每次寻找的单词不同，则可以维护⼀个哈希表 记录每个单词的下标列表。遍历⼀次⽂件，按照下标递增顺序得到每个单词在⽂件中出现的所有下 标。在寻找单词时，只要得到两个单词的下标列表，使⽤双指针遍历两个下标链表，即可得到两个 单词的最短距离

## 从10亿个数字中寻找最小的100万个数字

大顶堆

## 大数据TOP K总结

⾸先，我们来举⼏个常⻅的 topK 问题的例⼦：

1. 给定 100 个 int 数字，在其中找出最⼤的 10 个； 
2. 给定 10 亿个 int 数字，在其中找出最⼤的 10 个（这 10 个数字可以⽆序）； 
3. 给定 10 亿个 int 数字，在其中找出最⼤的 10 个（这 10 个数字依次排序）； 
4. 给定 10 亿个不重复的 int 数字，在其中找出最⼤的 10 个； 
5. 给定 10 个数组，每个数组中有 1 亿个 int 数字，在其中找出最⼤的 10 个； 
6. 给定 10 亿个 string 类型的数字，在其中找出最⼤的 10 个（仅需要查 1 次）； 
7. 给定 10 亿个 string 类型的数字，在其中找出最⼤的 k 个（需要反复多次查询，其中 k 是⼀个随机 数字）。

解决方法也有很多 : 

1. 堆排序(有序)
2. 快排(无序)
3. bitmap然后倒序遍历查看


